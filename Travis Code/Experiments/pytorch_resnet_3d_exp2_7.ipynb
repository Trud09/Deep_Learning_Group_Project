{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"pytorch_resnet_3d_exp2_7.ipynb","provenance":[],"collapsed_sections":[],"machine_shape":"hm"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU","widgets":{"application/vnd.jupyter.widget-state+json":{"8010eecd3d804bb7a09396c0b32e35b9":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","state":{"_view_name":"HBoxView","_dom_classes":[],"_model_name":"HBoxModel","_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.5.0","box_style":"","layout":"IPY_MODEL_d43e1c3dd91240c5ac083753e6688bae","_model_module":"@jupyter-widgets/controls","children":["IPY_MODEL_f440d78dc970459da268b0325d57c3c8","IPY_MODEL_929ae727ff2b400e96e4d98b2947c9f1"]}},"d43e1c3dd91240c5ac083753e6688bae":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"f440d78dc970459da268b0325d57c3c8":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","state":{"_view_name":"ProgressView","style":"IPY_MODEL_b05921159edc41bc9de3c0b268a74c0c","_dom_classes":[],"description":"100%","_model_name":"FloatProgressModel","bar_style":"success","max":108857766,"_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","value":108857766,"_view_count":null,"_view_module_version":"1.5.0","orientation":"horizontal","min":0,"description_tooltip":null,"_model_module":"@jupyter-widgets/controls","layout":"IPY_MODEL_324addcffba3406a865c26bc3ee97f7d"}},"929ae727ff2b400e96e4d98b2947c9f1":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","state":{"_view_name":"HTMLView","style":"IPY_MODEL_4ed8f4328f0240188e93ece5486d661f","_dom_classes":[],"description":"","_model_name":"HTMLModel","placeholder":"â€‹","_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","value":" 104M/104M [00:10&lt;00:00, 10.7MB/s]","_view_count":null,"_view_module_version":"1.5.0","description_tooltip":null,"_model_module":"@jupyter-widgets/controls","layout":"IPY_MODEL_f4e73846baeb4d6785ee31a4df99c0c8"}},"b05921159edc41bc9de3c0b268a74c0c":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","state":{"_view_name":"StyleView","_model_name":"ProgressStyleModel","description_width":"initial","_view_module":"@jupyter-widgets/base","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.2.0","bar_color":null,"_model_module":"@jupyter-widgets/controls"}},"324addcffba3406a865c26bc3ee97f7d":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"4ed8f4328f0240188e93ece5486d661f":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","state":{"_view_name":"StyleView","_model_name":"DescriptionStyleModel","description_width":"","_view_module":"@jupyter-widgets/base","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.2.0","_model_module":"@jupyter-widgets/controls"}},"f4e73846baeb4d6785ee31a4df99c0c8":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}}}}},"cells":[{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"0zElu3AK7_E-","outputId":"6c877870-b132-4879-d734-1374f04a0033"},"source":["from google.colab import drive\n","drive.mount('/content/drive')"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Mounted at /content/drive\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"dnqqy-uHP4UL"},"source":["root_dir = '/content/drive/MyDrive/Models_exp2.7'"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"p3FtG02x-G-i"},"source":["import os\n","import multiprocessing\n","import numpy as np\n","import cv2\n","import time\n","import shutil\n","\n","import torch\n","import torch.nn as nn\n","import torch.optim as optim\n","import torchvision\n","from torchvision import transforms"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"j3T5TELUrnrk","outputId":"e4db4703-257c-4a8e-a1b4-50536db091c7"},"source":["if torch.cuda.is_available():  \n","  dev = \"cuda:0\" \n","else:  \n","  dev = \"cpu\"  \n","\n","device = torch.device(dev)\n","print(device)"],"execution_count":null,"outputs":[{"output_type":"stream","text":["cuda:0\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"wSh4BhP7mT_K","outputId":"82ac635e-4823-4dfb-869f-1e53ed577f38"},"source":["!nvidia-smi"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Thu Apr 29 04:08:21 2021       \n","+-----------------------------------------------------------------------------+\n","| NVIDIA-SMI 465.19.01    Driver Version: 460.32.03    CUDA Version: 11.2     |\n","|-------------------------------+----------------------+----------------------+\n","| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n","| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n","|                               |                      |               MIG M. |\n","|===============================+======================+======================|\n","|   0  Tesla P100-PCIE...  Off  | 00000000:00:04.0 Off |                    0 |\n","| N/A   34C    P0    28W / 250W |      2MiB / 16280MiB |      0%      Default |\n","|                               |                      |                  N/A |\n","+-------------------------------+----------------------+----------------------+\n","                                                                               \n","+-----------------------------------------------------------------------------+\n","| Processes:                                                                  |\n","|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n","|        ID   ID                                                   Usage      |\n","|=============================================================================|\n","|  No running processes found                                                 |\n","+-----------------------------------------------------------------------------+\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"x4iQnfxqE1Dv"},"source":["#https://towardsdatascience.com/implementing-the-new-state-of-the-art-mish-activation-with-2-lines-of-code-in-pytorch-e7ef438a5ee7\n","def mish(x): \n","  return (x * torch.tanh(nn.functional.softplus(x)))"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"82GgC7MtuuVT"},"source":["#https://machinelearningmastery.com/pytorch-tutorial-develop-deep-learning-models/\n","class Model_3d(nn.Module):\n","    '''\n","    A model to use on 3d objects\n","    '''\n","    def __init__(self, fc_nodes, no_views, no_classes):\n","      super(Model_3d, self).__init__()\n","      self.resnet = torchvision.models.shufflenet_v2_x2_0(pretrained=True)\n","\n","      num_features = self.resnet.fc.out_features\n","\n","      self.fcs = nn.ModuleList()\n","\n","      for idx in range(no_views):\n","        self.fcs.append(nn.Linear(num_features, fc_nodes))\n","\n","      first_dense_count = fc_nodes * no_views\n","\n","      self.dense_1 = nn.Linear(first_dense_count, no_classes)\n","\n","    \n","    def forward(self, x):\n","      features = []\n","\n","      temp = x.view([-1, 3, 224, 224])\n","      \n","      temp =  nn.functional.interpolate(temp, 299)\n","\n","      x = temp.view([x.shape[0], x.shape[1], 3, 299, 299])\n","\n","      for i, fc in enumerate(self.fcs):\n","        sample = x[:, i, ...]\n","        r_out = self.resnet(sample)\n","        fc_out = fc(r_out)\n","        features.append(fc_out)\n","      \n","      stack = torch.stack(features, 1)\n","\n","      reshaped = stack.view([x.shape[0], -1])\n","\n","      x = self.dense_1(reshaped)\n","\n","\n","      return torch.nn.functional.softmax(x)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"mYjd0X-wusEO"},"source":["class Date_3d_Cached(torch.utils.data.Dataset):\n","    def __init__(self, x, y):\n","      self.x = torch.from_numpy(x)\n","      self.y = torch.from_numpy(y)\n","    \n","    def __len__(self):\n","      return self.x.shape[0]\n","\n","    def __getitem__(self, idx):\n","      x = self.x[idx] /255.0\n","      return x, self.y[idx]"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"Q6OEqBRm-r4e"},"source":["views = ['bottom', 'side_1', 'side_2', 'side_3', 'side_4', 'top', 'top_1', 'top_2', 'top_3', 'top_4', 'bot_1', 'bot_2', 'bot_3', 'bot_4']\n","x = np.load('/content/drive/MyDrive/Cache/x_compressed.npz')['arr_0']\n","y = np.load('/content/drive/MyDrive/Cache/y.npy')"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"Pk_DnyqsfjTV"},"source":["training_data = Date_3d_Cached(x, y)\n","#80/20 train val split\n","train_len = int(len(training_data) * .9)\n","val_len = len(training_data) - train_len"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"BhCvEGYjdQSj"},"source":["train_set, val_set = torch.utils.data.random_split(training_data, [train_len, val_len],torch.Generator().manual_seed(42))"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"WvuyARAKthBB"},"source":["batch_size = 8"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":105,"referenced_widgets":["8010eecd3d804bb7a09396c0b32e35b9","d43e1c3dd91240c5ac083753e6688bae","f440d78dc970459da268b0325d57c3c8","929ae727ff2b400e96e4d98b2947c9f1","b05921159edc41bc9de3c0b268a74c0c","324addcffba3406a865c26bc3ee97f7d","4ed8f4328f0240188e93ece5486d661f","f4e73846baeb4d6785ee31a4df99c0c8"]},"id":"3-Fm5OSjVGlm","outputId":"2edbcf02-0ce0-4af2-c19f-c391639344eb"},"source":["model = Model_3d(1024, len(views), 10)\n","model.to(device)\n","print(device)\n","training_loader = torch.utils.data.DataLoader(train_set, batch_size= batch_size, shuffle=True, num_workers=os.cpu_count())\n","val_loader = torch.utils.data.DataLoader(val_set, batch_size= batch_size, shuffle=True, num_workers=os.cpu_count())\n","\n","#https://pytorch.org/tutorials/beginner/blitz/cifar10_tutorial.html\n","criterion = nn.CrossEntropyLoss()\n","optimizer = optim.SGD(model.parameters(), lr=0.001, momentum=0.9)"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Downloading: \"https://download.pytorch.org/models/inception_v3_google-1a9a5a14.pth\" to /root/.cache/torch/hub/checkpoints/inception_v3_google-1a9a5a14.pth\n"],"name":"stderr"},{"output_type":"display_data","data":{"application/vnd.jupyter.widget-view+json":{"model_id":"8010eecd3d804bb7a09396c0b32e35b9","version_minor":0,"version_major":2},"text/plain":["HBox(children=(FloatProgress(value=0.0, max=108857766.0), HTML(value='')))"]},"metadata":{"tags":[]}},{"output_type":"stream","text":["\n","cuda:0\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"4nIuLwfCue8c","outputId":"ae510472-650b-4f71-896e-e7e8cabf78e5"},"source":["print(os.cpu_count())\n","print(train_len)\n","print(val_len)"],"execution_count":null,"outputs":[{"output_type":"stream","text":["4\n","3192\n","799\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"EY-fG1QaIPmk"},"source":["#https://medium.com/analytics-vidhya/saving-and-loading-your-model-to-resume-training-in-pytorch-cb687352fa61\n","def save_ckp(state, model_path):\n","    torch.save(state, model_path)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"iVH1Z0V6Hpn1","outputId":"8a623dc1-5c1d-4062-b8f4-d1f15b1ccb5c"},"source":["\n","\n","if not os.path.exists(root_dir):\n","    os.makedirs(root_dir)\n","\n","\n","low_val_loss = 30000000\n","best_acc = -1\n","epoch = 0\n","\n","val_loss_path = None\n","acc_path = None\n","#https://stackoverflow.com/questions/8078330/csv-writing-within-loop\n","import csv\n","with open(os.path.join(root_dir, 'logs.csv'), 'w') as file_csv:\n","  writer=csv.writer(file_csv, delimiter=',',lineterminator='\\n',)\n","  writer.writerow(['epoch', 'loss', 'acc', 'val_loss', 'val_acc'])\n","\n","\n","  for epoch in range(1, 101):\n","      row = [epoch]\n","      is_best = False\n","      running_loss = 0.0\n","      correct = 0\n","      total = 0\n","\n","      model.train()\n","\n","      t0 = time.time()\n","      for i, data in enumerate(training_loader):\n","          # get the inputs; data is a list of [inputs, labels]\n","          inputs, labels = data\n","\n","          # for this_view in range(inputs.shape[1]):\n","          #   if np.random.randint(0, 4) == 0:\n","          #     inputs[:, this_view, ...] = 1.0\n","\n","          inputs = inputs.to(device)\n","          labels = labels.to(device)\n","\n","          # zero the parameter gradients\n","          optimizer.zero_grad()\n","\n","          # forward + backward + optimize\n","          outputs = model(inputs)\n","          loss = criterion(outputs, labels)\n","          loss.backward()\n","          optimizer.step()\n","\n","          # print statistics\n","          correct += (outputs.argmax(1) == labels).float().sum() #https://stackoverflow.com/questions/51503851/calculate-the-accuracy-every-epoch-in-pytorch\n","          total += labels.shape[0]\n","          running_loss += loss.item()\n","\n","      print('epoch', epoch)\n","      \n","      print('{} seconds'.format(time.time() - t0))\n","      print('loss', running_loss)\n","      row.append(running_loss)\n","      running_loss = 0.0\n","\n","      accuracy = 100 * correct / total\n","      print(\"Accuracy = {}\".format(accuracy))\n","      row.append(torch.IntTensor.item(accuracy))\n","      correct = 0\n","      total= 0\n","\n","      running_loss = 0.0\n","      correct = 0\n","      total = 0\n","\n","      optimizer.zero_grad()\n","      model.eval()\n","      t0 = time.time()\n","      with torch.no_grad():\n","          for i, data in enumerate(val_loader):\n","              \n","              # get the inputs; data is a list of [inputs, labels]\n","              inputs, labels = data\n","              inputs = inputs.to(device)\n","              labels = labels.to(device)        \n","\n","\n","              # forward + backward + optimize\n","              outputs = model(inputs)\n","              loss = criterion(outputs, labels)\n","\n","              correct += (outputs.argmax(1) == labels).float().sum() #https://stackoverflow.com/questions/51503851/calculate-the-accuracy-every-epoch-in-pytorch\n","              total += labels.shape[0]\n","              running_loss += loss.item()\n","\n","      print('{} seconds'.format(time.time() - t0))\n","      print('val loss', running_loss)\n","      row.append(running_loss)\n","      \n","\n","      accuracy = 100 * correct / total\n","      print(\" Val Accuracy = {}\".format(accuracy))\n","      row.append(torch.IntTensor.item(accuracy))\n","\n","      model_name = 'ep_' + str(epoch) + '_loss_' + str(running_loss) + '_acc_' + str(accuracy) + '.pt'\n","      full_model_path = os.path.join(root_dir, model_name)\n","      checkpoint = {\n","          'epoch': epoch + 1,\n","          'state_dict': model.state_dict(),\n","          'optimizer': optimizer.state_dict()\n","      }\n","      if running_loss < low_val_loss:\n","        if val_loss_path and os.path.exists(val_loss_path):\n","          open(val_loss_path, 'w').close() #overwrite and make the file blank instead - ref: https://stackoverflow.com/a/4914288/3553367\n","          os.remove(val_loss_path)\n","        val_loss_path = full_model_path.replace('.pt', 'val_loss_best.pt')\n","        save_ckp(model, val_loss_path)\n","        low_val_loss = running_loss\n","        print('new low loss')\n","\n","      if accuracy > best_acc:\n","        if acc_path and os.path.exists(acc_path):\n","          open(acc_path, 'w').close() #overwrite and make the file blank instead - ref: https://stackoverflow.com/a/4914288/3553367\n","          os.remove(acc_path)\n","        acc_path = full_model_path.replace('.pt', 'acc_best.pt')\n","        save_ckp(model, acc_path)\n","        best_acc = accuracy\n","        print('new best acc')\n","      full_model_path = os.path.join(root_dir, 'last.pt')\n","      save_ckp(checkpoint, full_model_path)\n","          \n","\n","      \n","\n","      correct = 0\n","      total= 0\n","      running_loss = 0.0\n","      writer.writerow(row)\n","  print('Finished Training')"],"execution_count":null,"outputs":[{"output_type":"stream","text":["/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:44: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n"],"name":"stderr"},{"output_type":"stream","text":["epoch 1\n","478.87920236587524 seconds\n","loss 691.1347668170929\n","Accuracy = 77.19298553466797\n","41.76733112335205 seconds\n","val loss 157.21357762813568\n"," Val Accuracy = 91.1138916015625\n","new low loss\n","new best acc\n","epoch 2\n","478.4271261692047 seconds\n","loss 616.3526686429977\n","Accuracy = 93.17042541503906\n","41.654518604278564 seconds\n","val loss 154.99869751930237\n"," Val Accuracy = 92.99124145507812\n","new low loss\n","new best acc\n","epoch 3\n","478.34321427345276 seconds\n","loss 607.8149188756943\n","Accuracy = 94.5802001953125\n","41.66727614402771 seconds\n","val loss 153.2789980173111\n"," Val Accuracy = 93.2415542602539\n","new low loss\n","new best acc\n","epoch 4\n","478.2062888145447 seconds\n","loss 600.1790393590927\n","Accuracy = 96.55388641357422\n","41.70685815811157 seconds\n","val loss 151.7482876777649\n"," Val Accuracy = 94.86858367919922\n","new low loss\n","new best acc\n","epoch 5\n","478.92221426963806 seconds\n","loss 598.3700921535492\n","Accuracy = 96.77318572998047\n","41.458654165267944 seconds\n","val loss 150.87384867668152\n"," Val Accuracy = 95.6195297241211\n","new low loss\n","new best acc\n","epoch 6\n","478.46363377571106 seconds\n","loss 593.8229794502258\n","Accuracy = 97.83834838867188\n","41.583242416381836 seconds\n","val loss 151.77135479450226\n"," Val Accuracy = 94.49311828613281\n","epoch 7\n","478.6030738353729 seconds\n","loss 593.2456390857697\n","Accuracy = 97.83834838867188\n","41.7263560295105 seconds\n","val loss 153.5911659002304\n"," Val Accuracy = 93.11639404296875\n","epoch 8\n","478.6802034378052 seconds\n","loss 591.988666176796\n","Accuracy = 98.08897399902344\n","41.60568618774414 seconds\n","val loss 151.67462158203125\n"," Val Accuracy = 94.49311828613281\n","epoch 9\n","477.89024925231934 seconds\n","loss 591.1860492229462\n","Accuracy = 98.40225219726562\n","41.57855463027954 seconds\n","val loss 151.47599458694458\n"," Val Accuracy = 94.7434310913086\n","epoch 10\n","477.9362645149231 seconds\n","loss 588.4266413450241\n","Accuracy = 98.87217712402344\n","41.66977500915527 seconds\n","val loss 151.40817844867706\n"," Val Accuracy = 94.86858367919922\n","epoch 11\n","477.922660112381 seconds\n","loss 588.1458847522736\n","Accuracy = 98.9661636352539\n","41.79829239845276 seconds\n","val loss 151.6956262588501\n"," Val Accuracy = 94.61827087402344\n","epoch 12\n","478.18302631378174 seconds\n","loss 588.1302655935287\n","Accuracy = 98.99748992919922\n","41.62206530570984 seconds\n","val loss 150.8396908044815\n"," Val Accuracy = 95.86984252929688\n","new low loss\n","new best acc\n","epoch 13\n","478.57702565193176 seconds\n","loss 588.0205872058868\n","Accuracy = 99.02882385253906\n","41.81605672836304 seconds\n","val loss 153.09527730941772\n"," Val Accuracy = 93.11639404296875\n","epoch 14\n","478.6875877380371 seconds\n","loss 587.8407809734344\n","Accuracy = 99.06015014648438\n","41.7123863697052 seconds\n","val loss 151.7060590982437\n"," Val Accuracy = 94.49311828613281\n","epoch 15\n","478.17159605026245 seconds\n","loss 586.4549090862274\n","Accuracy = 99.31077575683594\n","41.55652618408203 seconds\n","val loss 153.2199683189392\n"," Val Accuracy = 93.36670684814453\n","epoch 16\n","478.20385217666626 seconds\n","loss 586.9972108602524\n","Accuracy = 99.21678924560547\n","41.658281564712524 seconds\n","val loss 151.71873712539673\n"," Val Accuracy = 95.118896484375\n","epoch 17\n","478.9431223869324 seconds\n","loss 586.3421992063522\n","Accuracy = 99.4047622680664\n","41.78747534751892 seconds\n","val loss 152.14075255393982\n"," Val Accuracy = 93.99249267578125\n","epoch 18\n","478.643522977829 seconds\n","loss 585.8921021223068\n","Accuracy = 99.43608856201172\n","41.73440861701965 seconds\n","val loss 151.4602906703949\n"," Val Accuracy = 94.7434310913086\n","epoch 19\n","478.5441241264343 seconds\n","loss 586.0032913684845\n","Accuracy = 99.4047622680664\n","41.802279233932495 seconds\n","val loss 151.66209411621094\n"," Val Accuracy = 94.7434310913086\n","epoch 20\n","478.43877387046814 seconds\n","loss 585.8706784248352\n","Accuracy = 99.46741485595703\n","41.73429536819458 seconds\n","val loss 150.70871222019196\n"," Val Accuracy = 95.6195297241211\n","new low loss\n","epoch 21\n","478.4856562614441 seconds\n","loss 585.3326075077057\n","Accuracy = 99.59272766113281\n","41.68197441101074 seconds\n","val loss 151.21080589294434\n"," Val Accuracy = 94.99374389648438\n","epoch 22\n","477.9124858379364 seconds\n","loss 585.9818147420883\n","Accuracy = 99.34210205078125\n","41.824164390563965 seconds\n","val loss 151.16259908676147\n"," Val Accuracy = 95.118896484375\n","epoch 23\n","478.51630997657776 seconds\n","loss 585.7039872407913\n","Accuracy = 99.46741485595703\n","41.672133445739746 seconds\n","val loss 150.52970588207245\n"," Val Accuracy = 95.74468231201172\n","new low loss\n","epoch 24\n","477.71540689468384 seconds\n","loss 585.4476855993271\n","Accuracy = 99.49874877929688\n","41.63409185409546 seconds\n","val loss 150.62979245185852\n"," Val Accuracy = 95.9949951171875\n","new best acc\n","epoch 25\n","478.0534620285034 seconds\n","loss 585.1709368228912\n","Accuracy = 99.5614013671875\n","41.66084671020508 seconds\n","val loss 150.87811064720154\n"," Val Accuracy = 95.49436950683594\n","epoch 26\n","478.37665605545044 seconds\n","loss 585.1414011716843\n","Accuracy = 99.53007507324219\n","41.86593723297119 seconds\n","val loss 150.55134689807892\n"," Val Accuracy = 95.74468231201172\n","epoch 27\n","478.8738052845001 seconds\n","loss 585.0088976621628\n","Accuracy = 99.59272766113281\n","41.86736845970154 seconds\n","val loss 150.1351934671402\n"," Val Accuracy = 95.9949951171875\n","new low loss\n","epoch 28\n","478.7307710647583 seconds\n","loss 584.9187022447586\n","Accuracy = 99.65538787841797\n","41.72800397872925 seconds\n","val loss 150.22212862968445\n"," Val Accuracy = 95.9949951171875\n","epoch 29\n","478.277117729187 seconds\n","loss 585.0645389556885\n","Accuracy = 99.59272766113281\n","41.60133147239685 seconds\n","val loss 151.13934564590454\n"," Val Accuracy = 95.24405670166016\n","epoch 30\n","478.6292955875397 seconds\n","loss 585.1226454973221\n","Accuracy = 99.59272766113281\n","41.71715497970581 seconds\n","val loss 150.80806958675385\n"," Val Accuracy = 95.24405670166016\n","epoch 31\n","477.8595962524414 seconds\n","loss 585.5271176099777\n","Accuracy = 99.49874877929688\n","41.93901562690735 seconds\n","val loss 151.09392833709717\n"," Val Accuracy = 95.49436950683594\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"vl2LSt99Vmqy"},"source":["import pandas as pd\n","     \n","# dictionary of lists  \n","this_dict = {'training loss': training_loss_list, 'training acc': training_acc_list, 'val loss': val_loss_list, 'val acc':val_acc_list}  \n","       \n","df = pd.DataFrame(this_dict) \n","    \n","# saving the dataframe \n","df.to_csv(os.path.join(root_dir, 'logs.csv'))"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"xM_87Dujyeat"},"source":["print('saved csv')"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"mWuip_oEF_of"},"source":["print(inputs.shape)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"-8MAE2jwG98_"},"source":["cp = torch.load(os.path.join(root_dir, 'last.pt'))\n","model.load_state_dict(cp['state_dict'])\n","optimizer.load_state_dict(cp['optimizer'])\n","last_epoch_done = cp['epoch'] - 1"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"qZES5XQaV8uA","outputId":"0246ff03-5006-44c5-c4d6-6b6b4e5998d4"},"source":["#continue training\n","\n","\n","if not os.path.exists(root_dir):\n","    os.makedirs(root_dir)\n","\n","\n","low_val_loss = 30000000\n","best_acc = -1\n","epoch = 0\n","\n","val_loss_path = None\n","acc_path = None\n","#https://stackoverflow.com/questions/8078330/csv-writing-within-loop\n","import csv\n","with open(os.path.join(root_dir, 'logs2.csv'), 'w') as file_csv:\n","  writer=csv.writer(file_csv, delimiter=',',lineterminator='\\n',)\n","  writer.writerow(['epoch', 'loss', 'acc', 'val_loss', 'val_acc'])\n","\n","\n","  for epoch in range(last_epoch_done, 101):\n","      row = [epoch]\n","      is_best = False\n","      running_loss = 0.0\n","      correct = 0\n","      total = 0\n","\n","      model.train()\n","\n","      t0 = time.time()\n","      for i, data in enumerate(training_loader):\n","          # get the inputs; data is a list of [inputs, labels]\n","          inputs, labels = data\n","\n","          # for this_view in range(inputs.shape[1]):\n","          #   if np.random.randint(0, 4) == 0:\n","          #     inputs[:, this_view, ...] = 1.0\n","\n","          inputs = inputs.to(device)\n","          labels = labels.to(device)\n","\n","          # zero the parameter gradients\n","          optimizer.zero_grad()\n","\n","          # forward + backward + optimize\n","          outputs = model(inputs)\n","          loss = criterion(outputs, labels)\n","          loss.backward()\n","          optimizer.step()\n","\n","          # print statistics\n","          correct += (outputs.argmax(1) == labels).float().sum() #https://stackoverflow.com/questions/51503851/calculate-the-accuracy-every-epoch-in-pytorch\n","          total += labels.shape[0]\n","          running_loss += loss.item()\n","\n","      print('epoch', epoch)\n","      \n","      print('{} seconds'.format(time.time() - t0))\n","      print('loss', running_loss)\n","      row.append(running_loss)\n","      running_loss = 0.0\n","\n","      accuracy = 100 * correct / total\n","      print(\"Accuracy = {}\".format(accuracy))\n","      row.append(torch.IntTensor.item(accuracy))\n","      correct = 0\n","      total= 0\n","\n","      running_loss = 0.0\n","      correct = 0\n","      total = 0\n","\n","      optimizer.zero_grad()\n","      model.eval()\n","      t0 = time.time()\n","      with torch.no_grad():\n","          for i, data in enumerate(val_loader):\n","              \n","              # get the inputs; data is a list of [inputs, labels]\n","              inputs, labels = data\n","              inputs = inputs.to(device)\n","              labels = labels.to(device)            \n","\n","\n","              # forward + backward + optimize\n","              outputs = model(inputs)\n","              loss = criterion(outputs, labels)\n","\n","              correct += (outputs.argmax(1) == labels).float().sum() #https://stackoverflow.com/questions/51503851/calculate-the-accuracy-every-epoch-in-pytorch\n","              total += labels.shape[0]\n","              running_loss += loss.item()\n","\n","      print('{} seconds'.format(time.time() - t0))\n","      print('val loss', running_loss)\n","      row.append(running_loss)\n","      \n","\n","      accuracy = 100 * correct / total\n","      print(\" Val Accuracy = {}\".format(accuracy))\n","      row.append(torch.IntTensor.item(accuracy))\n","\n","      model_name = 'ep_' + str(epoch) + '_loss_' + str(running_loss) + '_acc_' + str(accuracy) + '.pt'\n","      full_model_path = os.path.join(root_dir, model_name)\n","      checkpoint = {\n","          'epoch': epoch + 1,\n","          'state_dict': model.state_dict(),\n","          'optimizer': optimizer.state_dict()\n","      }\n","      if running_loss < low_val_loss:\n","        if val_loss_path and os.path.exists(val_loss_path):\n","          open(val_loss_path, 'w').close() #overwrite and make the file blank instead - ref: https://stackoverflow.com/a/4914288/3553367\n","          os.remove(val_loss_path)\n","        val_loss_path = full_model_path.replace('.pt', 'val_loss_best.pt')\n","        save_ckp(model, val_loss_path)\n","        low_val_loss = running_loss\n","        print('new low loss')\n","\n","      if accuracy > best_acc:\n","        if acc_path and os.path.exists(acc_path):\n","          open(acc_path, 'w').close() #overwrite and make the file blank instead - ref: https://stackoverflow.com/a/4914288/3553367\n","          os.remove(acc_path)\n","        acc_path = full_model_path.replace('.pt', 'acc_best.pt')\n","        save_ckp(model, acc_path)\n","        best_acc = accuracy\n","        print('new best acc')\n","      full_model_path = os.path.join(root_dir, 'last.pt')\n","      save_ckp(checkpoint, full_model_path)\n","          \n","\n","      \n","\n","      correct = 0\n","      total= 0\n","      running_loss = 0.0\n","      writer.writerow(row)\n","  print('Finished Training')"],"execution_count":null,"outputs":[{"output_type":"stream","text":["/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:44: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n"],"name":"stderr"},{"output_type":"stream","text":["epoch 40\n","476.59018301963806 seconds\n","loss 584.2237929105759\n","Accuracy = 99.71804809570312\n","41.53430438041687 seconds\n","val loss 151.7080943584442\n"," Val Accuracy = 94.7434310913086\n","new low loss\n","new best acc\n","epoch 41\n","475.66677498817444 seconds\n","loss 584.4062347412109\n","Accuracy = 99.68671417236328\n","41.40487194061279 seconds\n","val loss 150.8498740196228\n"," Val Accuracy = 95.49436950683594\n","new low loss\n","new best acc\n","epoch 42\n","475.2177743911743 seconds\n","loss 584.394015789032\n","Accuracy = 99.68671417236328\n","41.464412450790405 seconds\n","val loss 150.92265403270721\n"," Val Accuracy = 95.24405670166016\n","epoch 43\n","475.23102498054504 seconds\n","loss 584.4626857042313\n","Accuracy = 99.68671417236328\n","41.58085322380066 seconds\n","val loss 150.65498852729797\n"," Val Accuracy = 95.6195297241211\n","new low loss\n","new best acc\n","epoch 44\n","475.23362922668457 seconds\n","loss 584.2617198228836\n","Accuracy = 99.71804809570312\n","41.421157121658325 seconds\n","val loss 151.31585955619812\n"," Val Accuracy = 94.7434310913086\n","epoch 45\n","475.23237919807434 seconds\n","loss 584.2530319690704\n","Accuracy = 99.71804809570312\n","41.388890743255615 seconds\n","val loss 150.40078914165497\n"," Val Accuracy = 95.9949951171875\n","new low loss\n","new best acc\n","epoch 46\n","475.4853353500366 seconds\n","loss 584.4826279878616\n","Accuracy = 99.71804809570312\n","41.47763228416443 seconds\n","val loss 150.22177398204803\n"," Val Accuracy = 95.49436950683594\n","new low loss\n","epoch 47\n","475.3954122066498 seconds\n","loss 584.2247743606567\n","Accuracy = 99.71804809570312\n","41.53870129585266 seconds\n","val loss 150.57079446315765\n"," Val Accuracy = 95.49436950683594\n","epoch 48\n","475.32505917549133 seconds\n","loss 584.4003566503525\n","Accuracy = 99.68671417236328\n","41.362027406692505 seconds\n","val loss 150.75436627864838\n"," Val Accuracy = 95.118896484375\n","epoch 49\n","475.29221868515015 seconds\n","loss 584.256663441658\n","Accuracy = 99.71804809570312\n","41.437095403671265 seconds\n","val loss 150.57138967514038\n"," Val Accuracy = 95.49436950683594\n","epoch 50\n","475.3347828388214 seconds\n","loss 584.4834042787552\n","Accuracy = 99.65538787841797\n","41.47946381568909 seconds\n","val loss 151.52801704406738\n"," Val Accuracy = 94.86858367919922\n","epoch 51\n","475.2209005355835 seconds\n","loss 584.2564638853073\n","Accuracy = 99.74937438964844\n","41.39860153198242 seconds\n","val loss 150.50502812862396\n"," Val Accuracy = 95.74468231201172\n","epoch 52\n","475.16166472435 seconds\n","loss 584.2550134658813\n","Accuracy = 99.74937438964844\n","41.49435377120972 seconds\n","val loss 150.66755986213684\n"," Val Accuracy = 95.36921691894531\n","epoch 53\n","475.17924761772156 seconds\n","loss 584.3100609779358\n","Accuracy = 99.71804809570312\n","41.5292329788208 seconds\n","val loss 150.23694479465485\n"," Val Accuracy = 95.86984252929688\n","epoch 54\n","475.1594181060791 seconds\n","loss 584.5175561904907\n","Accuracy = 99.68671417236328\n","41.57971405982971 seconds\n","val loss 150.1688550710678\n"," Val Accuracy = 96.24530792236328\n","new low loss\n","new best acc\n","epoch 55\n","475.5828061103821 seconds\n","loss 584.1744999885559\n","Accuracy = 99.74937438964844\n","41.43025350570679 seconds\n","val loss 150.84707164764404\n"," Val Accuracy = 95.118896484375\n","epoch 56\n","475.2744896411896 seconds\n","loss 584.3122671842575\n","Accuracy = 99.68671417236328\n","41.5163836479187 seconds\n","val loss 150.1459835767746\n"," Val Accuracy = 95.86984252929688\n","new low loss\n","epoch 57\n","475.0687346458435 seconds\n","loss 584.0790501832962\n","Accuracy = 99.78070068359375\n","41.49990272521973 seconds\n","val loss 150.01347494125366\n"," Val Accuracy = 96.24530792236328\n","new low loss\n","epoch 58\n","475.3852951526642 seconds\n","loss 584.0160353183746\n","Accuracy = 99.78070068359375\n","41.44029998779297 seconds\n","val loss 150.03022611141205\n"," Val Accuracy = 96.24530792236328\n","epoch 59\n","475.3612554073334 seconds\n","loss 584.0838152170181\n","Accuracy = 99.74937438964844\n","41.464494705200195 seconds\n","val loss 150.25353801250458\n"," Val Accuracy = 95.74468231201172\n","epoch 60\n","475.24455881118774 seconds\n","loss 583.9589759111404\n","Accuracy = 99.78070068359375\n","41.380818605422974 seconds\n","val loss 150.37052285671234\n"," Val Accuracy = 95.74468231201172\n","epoch 61\n","475.48816418647766 seconds\n","loss 583.959730386734\n","Accuracy = 99.78070068359375\n","41.402143716812134 seconds\n","val loss 150.35963678359985\n"," Val Accuracy = 95.6195297241211\n","epoch 62\n","475.3187439441681 seconds\n","loss 584.1565724611282\n","Accuracy = 99.71804809570312\n","41.53604340553284 seconds\n","val loss 150.01083326339722\n"," Val Accuracy = 96.24530792236328\n","new low loss\n","epoch 63\n","474.95439195632935 seconds\n","loss 583.9229187965393\n","Accuracy = 99.78070068359375\n","41.39026594161987 seconds\n","val loss 150.4220668077469\n"," Val Accuracy = 95.9949951171875\n","epoch 64\n","474.7960948944092 seconds\n","loss 584.2289171218872\n","Accuracy = 99.68671417236328\n","41.46054744720459 seconds\n","val loss 150.21333348751068\n"," Val Accuracy = 95.9949951171875\n","epoch 65\n","475.05427861213684 seconds\n","loss 584.452921628952\n","Accuracy = 99.65538787841797\n","41.35108947753906 seconds\n","val loss 150.3881574869156\n"," Val Accuracy = 95.86984252929688\n","epoch 66\n","474.83827018737793 seconds\n","loss 583.985429763794\n","Accuracy = 99.78070068359375\n","41.42580270767212 seconds\n","val loss 149.6971138715744\n"," Val Accuracy = 96.62078094482422\n","new low loss\n","new best acc\n","epoch 67\n","474.9588544368744 seconds\n","loss 584.0835180282593\n","Accuracy = 99.74937438964844\n","41.51425528526306 seconds\n","val loss 150.16249871253967\n"," Val Accuracy = 95.9949951171875\n","epoch 68\n","474.98391914367676 seconds\n","loss 584.0606857538223\n","Accuracy = 99.78070068359375\n","41.367701053619385 seconds\n","val loss 149.93502521514893\n"," Val Accuracy = 96.24530792236328\n","epoch 69\n","474.8292758464813 seconds\n","loss 584.2141389846802\n","Accuracy = 99.71804809570312\n","41.42473483085632 seconds\n","val loss 149.77502977848053\n"," Val Accuracy = 96.74593353271484\n","new best acc\n","epoch 70\n","475.0779221057892 seconds\n","loss 584.6094298362732\n","Accuracy = 99.65538787841797\n","41.520655393600464 seconds\n","val loss 151.03163170814514\n"," Val Accuracy = 94.86858367919922\n","epoch 71\n","474.94250655174255 seconds\n","loss 584.0464416742325\n","Accuracy = 99.78070068359375\n","41.412354707717896 seconds\n","val loss 150.70488512516022\n"," Val Accuracy = 95.36921691894531\n","epoch 72\n","474.64438462257385 seconds\n","loss 584.1357394456863\n","Accuracy = 99.71804809570312\n","41.399253606796265 seconds\n","val loss 150.67350041866302\n"," Val Accuracy = 95.49436950683594\n","epoch 73\n","474.99341773986816 seconds\n","loss 584.2596247196198\n","Accuracy = 99.71804809570312\n","41.421196699142456 seconds\n","val loss 150.70166397094727\n"," Val Accuracy = 95.49436950683594\n","epoch 74\n","474.7262306213379 seconds\n","loss 584.1828554868698\n","Accuracy = 99.74937438964844\n","41.39368033409119 seconds\n","val loss 150.8773535490036\n"," Val Accuracy = 95.36921691894531\n","epoch 75\n","475.0147578716278 seconds\n","loss 584.2119724750519\n","Accuracy = 99.71804809570312\n","41.45376372337341 seconds\n","val loss 150.65527617931366\n"," Val Accuracy = 95.74468231201172\n","epoch 76\n","474.9509837627411 seconds\n","loss 583.9994789361954\n","Accuracy = 99.74937438964844\n","41.359933853149414 seconds\n","val loss 150.59192740917206\n"," Val Accuracy = 95.74468231201172\n","epoch 77\n","474.8130564689636 seconds\n","loss 584.122031211853\n","Accuracy = 99.74937438964844\n","41.447070837020874 seconds\n","val loss 150.3294758796692\n"," Val Accuracy = 95.86984252929688\n","epoch 78\n","474.5900151729584 seconds\n","loss 584.1126041412354\n","Accuracy = 99.74937438964844\n","41.458264112472534 seconds\n","val loss 150.26918041706085\n"," Val Accuracy = 95.9949951171875\n","epoch 79\n","474.31844210624695 seconds\n","loss 584.0399880409241\n","Accuracy = 99.78070068359375\n","41.35309338569641 seconds\n","val loss 151.01994729042053\n"," Val Accuracy = 95.49436950683594\n","epoch 80\n","474.519460439682 seconds\n","loss 584.1389904022217\n","Accuracy = 99.74937438964844\n","41.35722255706787 seconds\n","val loss 150.37453365325928\n"," Val Accuracy = 95.9949951171875\n","epoch 81\n","474.530775308609 seconds\n","loss 584.0953184366226\n","Accuracy = 99.71804809570312\n","41.57329845428467 seconds\n","val loss 150.54107904434204\n"," Val Accuracy = 95.6195297241211\n","epoch 82\n","474.56429171562195 seconds\n","loss 584.1348013877869\n","Accuracy = 99.74937438964844\n","41.3777391910553 seconds\n","val loss 150.42273032665253\n"," Val Accuracy = 95.9949951171875\n","epoch 83\n","474.7905344963074 seconds\n","loss 583.937264084816\n","Accuracy = 99.78070068359375\n","41.38847780227661 seconds\n","val loss 149.98427772521973\n"," Val Accuracy = 96.12015533447266\n","epoch 84\n","474.67282032966614 seconds\n","loss 584.0180990695953\n","Accuracy = 99.78070068359375\n","41.31058120727539 seconds\n","val loss 150.4592809677124\n"," Val Accuracy = 95.6195297241211\n","epoch 85\n","474.4655861854553 seconds\n","loss 583.965607047081\n","Accuracy = 99.78070068359375\n","41.33965802192688 seconds\n","val loss 149.9919377565384\n"," Val Accuracy = 96.24530792236328\n","epoch 86\n","474.76112723350525 seconds\n","loss 583.9325184822083\n","Accuracy = 99.78070068359375\n","41.372360706329346 seconds\n","val loss 149.94306421279907\n"," Val Accuracy = 96.24530792236328\n","epoch 87\n","474.56717252731323 seconds\n","loss 583.9874770641327\n","Accuracy = 99.74937438964844\n","41.357383251190186 seconds\n","val loss 150.18283414840698\n"," Val Accuracy = 95.74468231201172\n","epoch 88\n","474.6055030822754 seconds\n","loss 583.9843980073929\n","Accuracy = 99.78070068359375\n","41.317467212677 seconds\n","val loss 151.08641350269318\n"," Val Accuracy = 94.99374389648438\n","epoch 89\n","474.6350302696228 seconds\n","loss 584.0112036466599\n","Accuracy = 99.78070068359375\n","41.415053367614746 seconds\n","val loss 150.44387805461884\n"," Val Accuracy = 95.74468231201172\n","epoch 90\n","474.89235281944275 seconds\n","loss 583.9900287389755\n","Accuracy = 99.78070068359375\n","41.341639041900635 seconds\n","val loss 150.31482064723969\n"," Val Accuracy = 95.74468231201172\n","epoch 91\n","474.65346026420593 seconds\n","loss 583.9316037893295\n","Accuracy = 99.78070068359375\n","41.343289613723755 seconds\n","val loss 150.64062201976776\n"," Val Accuracy = 95.49436950683594\n","epoch 92\n","474.63749527931213 seconds\n","loss 583.9454704523087\n","Accuracy = 99.78070068359375\n","41.44541049003601 seconds\n","val loss 150.53279197216034\n"," Val Accuracy = 95.74468231201172\n","epoch 93\n","474.88188672065735 seconds\n","loss 584.0101163387299\n","Accuracy = 99.78070068359375\n","41.41464948654175 seconds\n","val loss 150.41883456707\n"," Val Accuracy = 95.6195297241211\n","epoch 94\n","474.6748592853546 seconds\n","loss 583.9363148212433\n","Accuracy = 99.78070068359375\n","41.43079972267151 seconds\n","val loss 150.43281173706055\n"," Val Accuracy = 95.9949951171875\n","epoch 95\n","474.75789618492126 seconds\n","loss 583.8763928413391\n","Accuracy = 99.81202697753906\n","41.454089879989624 seconds\n","val loss 150.07942187786102\n"," Val Accuracy = 96.24530792236328\n","epoch 96\n","474.7710461616516 seconds\n","loss 583.8282340764999\n","Accuracy = 99.81202697753906\n","41.41249227523804 seconds\n","val loss 149.9857121706009\n"," Val Accuracy = 96.37046813964844\n","epoch 97\n","474.8684735298157 seconds\n","loss 583.8557507991791\n","Accuracy = 99.81202697753906\n","41.36866331100464 seconds\n","val loss 150.43762969970703\n"," Val Accuracy = 95.86984252929688\n","epoch 98\n","474.6861050128937 seconds\n","loss 583.7184761762619\n","Accuracy = 99.8433609008789\n","41.56217360496521 seconds\n","val loss 150.7143384218216\n"," Val Accuracy = 95.86984252929688\n","epoch 99\n","474.70042967796326 seconds\n","loss 583.7003885507584\n","Accuracy = 99.8433609008789\n","41.39463973045349 seconds\n","val loss 150.31717467308044\n"," Val Accuracy = 95.74468231201172\n","epoch 100\n","474.73008608818054 seconds\n","loss 583.6712993383408\n","Accuracy = 99.8433609008789\n","41.520092248916626 seconds\n","val loss 150.2319883108139\n"," Val Accuracy = 95.9949951171875\n","Finished Training\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"nWgdf5HtWFVl"},"source":[""],"execution_count":null,"outputs":[]}]}